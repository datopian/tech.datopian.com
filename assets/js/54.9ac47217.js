(window.webpackJsonp=window.webpackJsonp||[]).push([[54],{401:function(e,a,t){"use strict";t.r(a);var o=t(25),i=function(e){e.options.__data__block__={mermaid_1a962853:"graph LR\n\nsubgraph Orchestration\n  airflow[AirFlow]\n  airflowservice[AirFlow service]\nend\n\nsubgraph CKAN integration\n  ckanhooks[CKAN extension to trigger and report on factory activity]\n  ckanapi[API for triggering DAGs etc]\n  ckanui[UI integration - display info on ]\nend\n\nsubgraph Processors and Flows\n  ckandatastoreload[CKAN Loader lib]\n  ckanharveters[CKAN Harvesters]\n  validation[Validation Lib]\nend\n"}},s=Object(o.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"data-factory-design"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#data-factory-design"}},[e._v("#")]),e._v(" Data Factory Design")]),e._v(" "),t("p",[e._v("Our Data Factory system is called AirCan. A Data Factory is a set of services/components to process and integrate data (coming from different sources). Plus patterns / methods for integrating with CKAN and the DataHub.")]),e._v(" "),t("h2",{attrs:{id:"components"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#components"}},[e._v("#")]),e._v(" Components")]),e._v(" "),t("Mermaid",{attrs:{id:"mermaid_1a962853",graph:e.$dataBlock.mermaid_1a962853}}),t("h2",{attrs:{id:"datastore-load-job-story"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#datastore-load-job-story"}},[e._v("#")]),e._v(" DataStore Load job story")]),e._v(" "),t("h3",{attrs:{id:"reporting-integration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#reporting-integration"}},[e._v("#")]),e._v(" Reporting Integration")]),e._v(" "),t("p",[e._v("When I upload a file to CKAN and it is getting loaded to the datastore (automatically), I want to know if that succeeded or failed so that I can share with my users that the new data is available (or do something about the error).")]),e._v(" "),t("p",[e._v("For a remote Airflow instance (let’s say on Google Composer), describe the DAG tasks and the process. i.e.")]),e._v(" "),t("ul",[t("li",[e._v("File upload on CKAN triggers the ckanext-aircan connector")]),e._v(" "),t("li",[e._v("which makes API request to airflow on GCP and triggers a DAG with following parameters\n"),t("ul",[t("li",[e._v("A f11s resource orject including\n"),t("ul",[t("li",[e._v("the remote location of the CSV file and the reource ID")]),e._v(" "),t("li",[e._v("The target resource id")])])]),e._v(" "),t("li",[e._v("An API key to use when loading to CKAN datastore")]),e._v(" "),t("li",[e._v("[A callback url]")])])]),e._v(" "),t("li",[e._v("The DAG\n"),t("ul",[t("li",[e._v("deletes the datatore table")]),e._v(" "),t("li",[e._v("if it exists, creates a new datastore table")]),e._v(" "),t("li",[e._v("loads CSV from the specified location (inforation available on DAG parameters)")]),e._v(" "),t("li",[e._v("converts the CSV to JSON. The output of the converted JSON file will be in a bucket on GCP.")]),e._v(" "),t("li",[e._v("upserts the JSON data row by row into the CKAN DataStore via CKAN’s DataStore API\n"),t("ul",[t("li",[e._v("This is what we have now: invoke{“message”:“Created <DagRun ckan_api_load_gcp @ 2020-07-14 13:04:43+00:00: manual__2020-07-14T13:04:43+00:00, externally triggered: True>”} "),t("code",[e._v("/api/3/action/datastore_create")]),e._v(" and passing the contents of the json file\n"),t("ul",[t("li",[e._v("OR using upsert with inserts (faster) NB: datapusher just pushes the whole thing into "),t("code",[e._v("datastore_create")]),e._v(" so stick with that.")])])]),e._v(" "),t("li",[e._v("OR: if we are doing postgres copy we need direct access to postgres DB")])])]),e._v(" "),t("li",[e._v("… [tbd] notifies CKAN instance of this (?)")])])])]),e._v(" "),t("p",[e._v("Error Handling and other topics to consider")]),e._v(" "),t("ul",[t("li",[e._v("How can we let CKAN know something went wrong? Shall we create a way to notify a certain endpoint on ckannext-aircan connector?")]),e._v(" "),t("li",[e._v("Shall we also implement a timeout on CKAN?")]),e._v(" "),t("li",[e._v("What are we going to display in case of an error?")]),e._v(" "),t("li",[e._v("The “tmp” bucket on GCP will eventually get full of files; shall we flush it? How do we know when it’s safe to delete a file?\n"),t("ul",[t("li",[e._v("Lots of ways up this mountain.")])])]),e._v(" "),t("li",[e._v("What do we do for large files?")])]),e._v(" "),t("h2",{attrs:{id:"aircan-api"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aircan-api"}},[e._v("#")]),e._v(" AirCan API")]),e._v(" "),t("p",[e._v("AirCan is built on AirFlow so we have same basic API TODO: insert link")]),e._v(" "),t("p",[e._v("However, we have standard message formats to pass to DAGs following these principles: All dataset and data resource objects should following the Frictionless specs")]),e._v(" "),t("p",[e._v("Pseudo-code showing how we call the API:")]),e._v(" "),t("div",{staticClass:"language-python= extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('airflow.dag_run({\n  "conf": {\n    "resource": json.dumps({  # f11s resource object\n        resource_id: ...\n        path: ...\n        schema: ...\n    })\n    "ckan_api_key: ...\n    "ckan_api_endpoint": demo.ckan.org/api/\n  }\n})\n')])])]),t("p",[e._v("See for latest, up to date version: "),t("a",{attrs:{href:"https://github.com/datopian/ckanext-aircan/blob/master/ckanext/aircan_connector/action.py#L68",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/datopian/ckanext-aircan/blob/master/ckanext/aircan_connector/action.py#L68"),t("OutboundLink")],1)]),e._v(" "),t("h2",{attrs:{id:"ckan-integration-api"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ckan-integration-api"}},[e._v("#")]),e._v(" CKAN integration API")]),e._v(" "),t("p",[e._v("There is a new API as follows:")]),e._v(" "),t("p",[t("code",[e._v("http://ckan:5000/api/3/action/aircan_submit?dag_id=...&dataset=...&resource")])]),e._v(" "),t("p",[e._v("Also DAGs can get triggered on events … TODO: go look at Github actions and learn from it …")]),e._v(" "),t("h2",{attrs:{id:"architecture"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#architecture"}},[e._v("#")]),e._v(" Architecture")]),e._v(" "),t("p",[e._v("Other principles of architecture:")]),e._v(" "),t("ul",[t("li",[e._v("AirFlow tasks and DAGs should do very little themselves and should hand off to separate libraries. Why? To have better separation of concerns and "),t("strong",[e._v("testability")]),e._v(". AirCan is reasonably cumbersome to test but an SDK is much more testable.\n"),t("ul",[t("li",[e._v("Thus AirFlow tasks are often just going to pass through arguments TODO: expand this with an example …")])])]),e._v(" "),t("li",[e._v("AirFlow DAG will have incoming data and config set in “global” config for the DAG and so available to every task …")]),e._v(" "),t("li",[e._v("Tasks should be as decoupled as possible. Obviously there "),t("em",[e._v("is")]),e._v(" some data and metadata passing between tasks and that should be done by writing those to a storage bucket. Metadata MUST be stored in f11s format.\n"),t("ul",[t("li",[e._v("See this interesting blog post (not scientific) about why the previous approach, with side effcts, is not very resilient in the long run of a project "),t("a",{attrs:{href:"https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("don’t pass data explicitly between tasks (rather it is passed implicitly via an expectation of where the data is stored …)")]),e._v(" "),t("li",[e._v("tasks and flows should be re-runnable … (no side effects principle)")])])])]),e._v(" "),t("p",[e._v("Each task can write to this location:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("bucket/dagid/runid/taskid/resource.json\nbucket/dagid/runid/taskid/dataset.json\nbucket/dagid/runid/taskid/... # data files\n")])])]),t("h2",{attrs:{id:"ui-in-dms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ui-in-dms"}},[e._v("#")]),e._v(" UI in DMS")]),e._v(" "),t("p",[e._v("URL structure on a daaset")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("# xxx is a dataset\n/@myorg/xxx/actions/\n/@myorg/xxx/actions/runs/{id}\n")])])]),t("p",[e._v("Main question: to display to user we need some way to log what jobs are associated with what datasets (and users) and perhaps their status")]),e._v(" "),t("ul",[t("li",[e._v("we want to keep factory relatively dumb (it does not know about datasets etc etc)")]),e._v(" "),t("li",[e._v("in terms of capabilities we need a way to pass permissions into the data factory (you hand over the keys to your car)")])]),e._v(" "),t("p",[e._v("Simplest approach:")]),e._v(" "),t("ul",[t("li",[e._v("MetaStore (CKAN metadata db) has Jobs table which have structure of "),t("code",[e._v("| id | factory_id | job_type | created | updated | dataset | resource | user | status | info |")]),e._v(" (where info is json blob)\n"),t("ul",[t("li",[e._v("status = one of "),t("code",[e._v("WAITING | RUNNING | DONE | FAILED | CANCELLED")]),e._v(". If failed we should have stuff in info about that.")]),e._v(" "),t("li",[t("code",[e._v("job_type")]),e._v(" = one of "),t("code",[e._v("HARVEST | LOAD | VALIDATE ...")]),e._v(" it is there so we could have several different factory jobs in one db")]),e._v(" "),t("li",[t("code",[e._v("info")]),e._v(": likely stuff\n"),t("ul",[t("li",[e._v("run time")]),e._v(" "),t("li",[e._v("error information (on failure)")]),e._v(" "),t("li",[e._v("success information: what was outcome, where are outputs if any etc")])])])])]),e._v(" "),t("li",[e._v("On creating a job in the factory, the factory returns a factory id. the metastore stores the factory id into a new job object along with dataset and user info …\n"),t("ul",[t("li",[e._v("Qu: why have id and factory_id separate? is there any situation where you have a job w/o a factory id?")])])]),e._v(" "),t("li",[e._v("Then on loading a job page in frontend you can poll the factory for info and status (if status is WAITING or RUNNING)\n"),t("ul",[t("li",[e._v("=> do we need the "),t("code",[e._v("info")]),e._v(" column on the job (it’s just a cache of this info)?\n"),t("ul",[t("li",[e._v("Ans: useful for jobs which are complete so we don’t keep polling the factory (esp if factory deletes stuff)")])])])])]),e._v(" "),t("li",[e._v("Can list all jobs for a given dataset (or resource) with info about them")])]),e._v(" "),t("p",[e._v("Qus:")]),e._v(" "),t("ul",[t("li",[e._v("For Data Factory what do I do with Runs that are stale etc - how do I know who they are associated with. Can I store metadata on my Runs like who requested it etc.")])]),e._v(" "),t("h2",{attrs:{id:"appendix"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#appendix"}},[e._v("#")]),e._v(" Appendix")]),e._v(" "),t("h3",{attrs:{id:"notes-re-aircan-api"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#notes-re-aircan-api"}},[e._v("#")]),e._v(" Notes re AirCan API")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://medium.com/@ptariche/interact-with-apache-airflows-experimental-api-3eba195f2947",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://medium.com/@ptariche/interact-with-apache-airflows-experimental-api-3eba195f2947"),t("OutboundLink")],1)]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('{"message":"Created <DagRun ckan_api_load_gcp @ 2020-07-14 13:04:43+00:00: manual__2020-07-14T13:04:43+00:00, externally triggered: True>"}\n\nGET /api/experimental/dags/<string:dag_id>/dag_runs/<string:execution_date>\n\nGET /api/experimental/dags/ckan_api_load_gcp/dag_runs/2020-07-14 13:04:43+00:00\n\nhttps://b011229e45c662be6p-tp.appspot.com/api/experimental/dags/ckan_api_load_gcp/dag_runs/2020-07-14T13:04:43+00:00\n\nResp: `{"state":"failed"}`\n')])])]),t("h3",{attrs:{id:"google-cloud-composer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#google-cloud-composer"}},[e._v("#")]),e._v(" Google Cloud Composer")]),e._v(" "),t("p",[e._v("Google Cloud Composer is a hosted version of AirFlow on Google Cloud.")]),e._v(" "),t("h4",{attrs:{id:"how-google-cloud-composer-differs-from-local-airflow"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#how-google-cloud-composer-differs-from-local-airflow"}},[e._v("#")]),e._v(" How Google Cloud Composer differs from local AirFlow")]),e._v(" "),t("ul",[t("li",[e._v("File handling: On GCP, all the file handling must become interaction with a bucket ~rufus: what about from a url online (but not a bucket)"),t("br"),e._v("\nSpecifying the csv resource location (on a local Airflow) must become sending a resource to a bucket (or just parsing it from the JSON body). When converting it to a JSON file, it must become an action of creating a file on a bucket.")]),e._v(" "),t("li",[e._v("Authentication: TODO")])]),e._v(" "),t("h3",{attrs:{id:"airflow-best-practices"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#airflow-best-practices"}},[e._v("#")]),e._v(" AirFlow Best Practices")]),e._v(" "),t("ul",[t("li",[e._v("Should you and how do you pass information between tasks?\n"),t("ul",[t("li",[t("a",{attrs:{href:"https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://towardsdatascience.com/airflow-sharing-data-between-tasks-7bbaa27eeb1",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://towardsdatascience.com/airflow-sharing-data-between-tasks-7bbaa27eeb1"),t("OutboundLink")],1)])])])]),e._v(" "),t("h3",{attrs:{id:"what-terminology-should-we-use"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#what-terminology-should-we-use"}},[e._v("#")]),e._v(" What terminology should we use?")]),e._v(" "),t("p",[e._v("ANS: we use AirFlow terminology:")]),e._v(" "),t("ul",[t("li",[e._v("Task")]),e._v(" "),t("li",[e._v("DAG")]),e._v(" "),t("li",[e._v("DagRun")])]),e._v(" "),t("p",[e._v("For internals what are the options?")]),e._v(" "),t("ul",[t("li",[e._v("Task or Processor or …")]),e._v(" "),t("li",[e._v("DAG or Flow or Pipeline?")])]),e._v(" "),t("p",[e._v("TODO: table summarizing options in AirFlow, Luigi, Apache Beam etc.")]),e._v(" "),t("h4",{attrs:{id:"ui-terminology"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ui-terminology"}},[e._v("#")]),e._v(" UI Terminology")]),e._v(" "),t("ul",[t("li",[e._v("Actions")]),e._v(" "),t("li",[e._v("Workflows")])]),e._v(" "),t("p",[e._v("Terminology options")]),e._v(" "),t("ul",[t("li",[e._v("Gitlab\n"),t("ul",[t("li",[e._v("Pipelines: you have")]),e._v(" "),t("li",[e._v("Jobs (runs of those")]),e._v(" "),t("li",[e._v("Schedules")])])]),e._v(" "),t("li",[e._v("Github\n"),t("ul",[t("li",[e._v("Workflows")]),e._v(" "),t("li",[e._v("Runs")]),e._v(" "),t("li",[e._v("(Schedules - not explicit)")])])]),e._v(" "),t("li",[e._v("Airflow\n"),t("ul",[t("li",[e._v("DAGs\n"),t("ul",[t("li",[e._v("Tasks")])])]),e._v(" "),t("li",[e._v("DAG Runs")])])])])],1)}),[],!1,null,null,null);"function"==typeof i&&i(s);a.default=s.exports}}]);