(window.webpackJsonp=window.webpackJsonp||[]).push([[52],{377:function(e,a,n){"use strict";n.r(a);var r=n(18),s=function(e){e.options.__data__block__={mermaid_64a5705e:"graph TD\n\nflow[Flow]\npipeline[Pipelines]\nprocessor[Processors]\n\nflow --\x3e pipeline\npipeline --\x3e processor\n",mermaid_64a5702c:"graph LR\n\nsource[Source from  User<br/>source.yaml] --assembler--\x3e plan[Execution Plan<br/>DAG of pipelines]\nplan --\x3e pipeline[Pipeline Runner<br/>Optimizer/Dependency management]\n",mermaid_64a56f38:"graph LR\n\nsource[Source Spec Parse<br/>validate?] --\x3e planner[Planner]\nplanner --\x3e workplanb(Plan of Work<br/><br/>DAG of pipelines)\n\nsubgraph Planner\n  planner\n  subplan1[Sub Planner 1 e.g. SQLite]\nend\n",mermaid_64a568e0:"graph TD\n\nflow[Flow]\npipeline[Pipelines]\nprocessor[Processors]\n\nflow --\x3e pipeline\npipeline --\x3e processor\n",mermaid_64a5676e:"graph LR\n\nsubgraph flow2\nx --\x3e z\ny --\x3e z\nend\n\nsubgraph flow1\na --\x3e c\nb --\x3e c\nend\n",mermaid_64a5599c:'graph LR\n\nsubgraph Source\n  file[vix-daily.csv]\n  view[timeseries-1<br/>Uses vix-daily]\nend\n\nsubgraph "Generated Resources"\n  gcsv[derived/vix-daily.csv]\n  gjson[derived/vix-daily.json]\n  gjsonpre[derived/vix-daily-10k.json]\n  gjsonpre2[derived/view-time-series-1.json]\nend\n\nsubgraph "Generated Views"\n  preview[Table Preview]\nend\n\nfile --rule--\x3e gcsv\nfile --rule--\x3e gjson\nfile --rule--\x3e preview\n\nview --\x3e gjsonpre2\npreview --\x3e gjsonpre\n',mermaid_64a55964:'graph LR\n\nload[Load Data Package<br/>Parse datapackage.json]\nparse[Parse source data]\ndump((S3))\n\nload --\x3e parse\nparse --\x3e dcsv\nparse --\x3e djson\nparse --\x3e dumpdp\ndcsv --\x3e dump\ndjson --\x3e dump\ndsqlite --\x3e dump\ndnode --\x3e dump\ndumpdp --\x3e dump\n\ndcsv --\x3e dnode\ndcsv --\x3e dsqlite\n\nsubgraph "Dumpers 1"\n  dcsv[Derived CSV]\n  djson[Derived JSON]\nend\n\nsubgraph "Dumpers 2 - after Dumper 1"\n  dsqlite[SQLite]\n  dnode[Node]\nend\n\n  dumpdp[Derived DataPackage.json<br/><br/>Assembler gives it the DAG info<br/>Runs after everything<br/>as needs size,md5 etc]\n',mermaid_64a5595a:'graph LR\n\nload[Load Data Package<br/>Parse datapackage.json]\nparse[Parse source data]\ndump((S3))\n\nload --\x3e parse\nparse --\x3e dcsv\nparse --\x3e djson\nparse --\x3e dumpdp\ndcsv --\x3e dump\ndjson --\x3e dump\ndsqlite --\x3e dump\ndnode --\x3e dump\ndumpdp --\x3e dump\n\ndcsv --\x3e dnode\ndcsv --\x3e dsqlite\n\nparse --\x3e viewgen\nviewgen --\x3e previewgen\npreviewgen --view-10k.json--\x3e dump\n\nsubgraph "Dumpers 1"\n  dcsv[Derived CSV]\n  djson[Derived JSON]\nend\n\nsubgraph "Dumpers 2 - after Dumper 1"\n  dsqlite[SQLite]\n  dnode[Node]\n  viewgen[Preview View Gen<br/><em>Adds preview views</em>]\n  previewgen[View Resource Generator]\nend\n\n  dumpdp[Derived DataPackage.json<br/><br/>Assembler gives it the DAG info<br/>Runs after everything<br/>as needs size,md5 etc]\n',mermaid_64a55922:'graph LR\n\nload[Load Data Package<br/>Parse datapackage.json]\nparse[Parse source data]\ndump((S3))\n\ndumpdp[Derived DataPackage.json<br/><br/>Assembler gives it the DAG info<br/>Runs after everything<br/>as needs size,md5 etc]\n\nload --\x3e parse\n\nparse --\x3e d1csv\nparse --\x3e d1json\nparse --\x3e d2csv\nparse --\x3e d2json\n\nparse --\x3e dumpdp\n\nd1csv --\x3e dump\nd1json --\x3e dump\nd2csv --\x3e dump\nd2json --\x3e dump\n\ndsqlite --\x3e dump\ndnode --\x3e dump\ndumpdp --\x3e dump\n\nd1csv --\x3e dnode\nd1csv --\x3e dsqlite\nd2csv --\x3e dnode\nd2csv --\x3e dsqlite\n\nd1csv --\x3e view1gen\nd2csv --\x3e view2gen\nview1gen --\x3e preview1gen\nview2gen --\x3e preview2gen\npreview1gen --view1-10k.json--\x3e dump\npreview2gen --view2-10k.json--\x3e dump\n\nsubgraph "Dumpers 1 sheet 1"\n  d1csv[Derived CSV]\n  d1json[Derived JSON]\nend\n\nsubgraph "Dumpers 1 sheet 2"\n  d2csv[Derived CSV]\n  d2json[Derived JSON]\nend\n\nsubgraph "Dumpers 2 - after Dumper 1"\n  dsqlite[SQLite]\n  dnode[Node]\n  view1gen[Preview View Gen<br/><em>Adds preview views</em>]\n  preview1gen[View Resource Generator]\n  view2gen[Preview View Gen<br/><em>Adds preview views</em>]\n  preview2gen[View Resource Generator]\nend\n',mermaid_64a558ac:"graph LR\n\nsource[Source from  User<br/>source.yaml] --assembler--\x3e plan[Execution Plan<br/>DAG of pipelines]\nplan --\x3e pipeline[Pipeline Runner<br/>Optimizer/Dependency management]\n",mermaid_64a552a2:"graph LR\n\nsource[Source Spec Parse<br/>validate?] --\x3e planner[Planner]\nplanner --\x3e workplanb(Plan of Work<br/><br/>DAG of pipelines)\n\nsubgraph Planner\n  planner\n  subplan1[Sub Planner 1 e.g. SQLite]\nend\n"}},t=Object(r.a)({},(function(){var e=this,a=e.$createElement,n=e._self._c||a;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"data-factory-and-flows-design-oct-2017-to-apr-2018"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#data-factory-and-flows-design-oct-2017-to-apr-2018"}},[e._v("#")]),e._v(" Data Factory and Flows Design - Oct 2017 to Apr 2018")]),e._v(" "),n("p",[e._v("Date: 2018-04-08")]),e._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[e._v("NOTE")]),e._v(" "),n("p",[e._v("This is a miscellaneous content from various HackMD docs. I’m preserving because either a) there is material to reuse here that I’m not sure is elsewhere b) there were various ideas in here we used later (and it’s useful to see their origins).")]),e._v(" "),n("p",[e._v("Key content:")]),e._v(" "),n("ul",[n("li",[e._v("March-April 2018: first planning of what became dataflows (had various names including dataos). A lot of my initial ideas ended up in this micro-demo "),n("a",{attrs:{href:"https://github.com/datopian/dataflow-demo",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/datopian/dataflow-demo"),n("OutboundLink")],1),e._v(". This evolved with Adam into "),n("a",{attrs:{href:"https://github.com/datahq/dataflows",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/datahq/dataflows"),n("OutboundLink")],1)]),e._v(" "),n("li",[e._v("Autumn 2017: planning of Data Factory which was the data processing system inside "),n("a",{attrs:{href:"http://DataHub.io",target:"_blank",rel:"noopener noreferrer"}},[e._v("DataHub.io"),n("OutboundLink")],1),e._v(". This was more extensive than dataflows (e.g. it included a runner, an assembler etc) and was based original data-package-pipelines and its runner. Issues with that system was part of the motivation for starting work on dataflows.")])]),e._v(" "),n("p",[e._v("~Rufus May 2020")])]),e._v(" "),n("h2",{attrs:{id:"plan-april-2018"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#plan-april-2018"}},[e._v("#")]),e._v(" Plan April 2018")]),e._v(" "),n("ul",[n("li",[e._v("tutorial (what we want our first post to look like)\n"),n("ul",[n("li",[e._v("And then implement minimum for that")])])]),e._v(" "),n("li",[e._v("programmatic use of pipelines and processors in DPP\n"),n("ul",[n("li",[e._v("processor abstraction defined …\n"),n("ul",[n("li",[e._v("DataResource and DataPackage object that looks like "),n("a",{attrs:{href:"http://okfnlabs.org/blog/2018/02/15/design-pattern-for-a-core-data-library.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Frictionless Lib pattern"),n("OutboundLink")],1)]),e._v(" "),n("li",[e._v("processors library split out")])])]),e._v(" "),n("li",[e._v("code runner you can call dataos.run.runSyncPipeline")])])]),e._v(" "),n("li",[e._v("dataflow init => python and yaml")]),e._v(" "),n("li",[e._v("@adam Write up Data Factory architecture and naming as it currently stands [2h]")])]),e._v(" "),n("h2",{attrs:{id:"_8-april-2018"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_8-april-2018"}},[e._v("#")]),e._v(" 8 April 2018")]),e._v(" "),n("p",[e._v("Lots of note on DataFlow which are now moved and refactored into "),n("a",{attrs:{href:"https://github.com/datahq/dataflow-demo",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/datahq/dataflow-demo"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("The Domain Model of Factory")]),e._v(" "),n("ul",[n("li",[e._v("Staging area")]),e._v(" "),n("li",[e._v("Planner")]),e._v(" "),n("li",[e._v("Runner")]),e._v(" "),n("li",[e._v("Flow\n"),n("ul",[n("li",[e._v("Pipelines")]),e._v(" "),n("li",[e._v("Processors")])])])]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a5705e",graph:e.$dataBlock.mermaid_64a5705e}}),n("p",[e._v("Assembler …")]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a5702c",graph:e.$dataBlock.mermaid_64a5702c}}),n("p",[e._v("=> Assembler generates a DAG.")]),e._v(" "),n("ul",[n("li",[e._v("dest filenames in advance …")]),e._v(" "),n("li",[e._v("for each pipeline: pipelines it dpeends on\n"),n("ul",[n("li",[e._v("e.g. sqlite: depends on on all derived csv pipelines running")]),e._v(" "),n("li",[e._v("node depends: all csv, all json pipelines running")]),e._v(" "),n("li",[e._v("zip: depends on all csv running")])])]),e._v(" "),n("li",[e._v("Pipelines")])]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a56f38",graph:e.$dataBlock.mermaid_64a56f38}}),n("h2",{attrs:{id:"_5-oct-2017"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_5-oct-2017"}},[e._v("#")]),e._v(" 5 Oct 2017")]),e._v(" "),n("p",[e._v("Notes:")]),e._v(" "),n("ul",[n("li",[e._v("Adam: finding more and more bugs (edge cases) and then applying fixes but then more issues\n"),n("ul",[n("li",[e._v("=> Internal data model of pipelines was wrong …")]),e._v(" "),n("li",[e._v("Original data model has a store: with one element the pipeline + a state (its idle, invalid, waiting to be executed, running, dirty)")]),e._v(" "),n("li",[e._v("Problem starts: you have a very long pipeline …\n"),n("ul",[n("li",[e._v("something changes and pipeline gets re-added to the queue. then you have same pipeline in queue in two different states. Should not be a state of the pipeline but state of the execution of the pipeline.")])])]),e._v(" "),n("li",[e._v("Split model: pipeline (with their hash) + “runs” ordered by time of request")])])])]),e._v(" "),n("p",[e._v("Questions:")]),e._v(" "),n("ul",[n("li",[e._v("Tests in assembler …")])]),e._v(" "),n("h3",{attrs:{id:"domain-model"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#domain-model"}},[e._v("#")]),e._v(" Domain Model")]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a568e0",graph:e.$dataBlock.mermaid_64a568e0}}),n("ul",[n("li",[e._v("Pipelines have no branches they are always linear\n"),n("ul",[n("li",[e._v("Input: nothing or a file or a datapackage (source is stream or nothing)")]),e._v(" "),n("li",[e._v("Output: datapackage - usually dumped to something (could be stream)")]),e._v(" "),n("li",[e._v("Pipelines are a list of processors "),n("strong",[e._v("and")]),e._v(" their inputs")])])]),e._v(" "),n("li",[e._v("A Flow is a DAG of pipelines\n"),n("ul",[n("li",[e._v("In our case: one flow produces a “Dataset” at a given “commit/run”")])])]),e._v(" "),n("li",[e._v("Source Spec + DataPackage[.json] => (via assembler) => Flow Spec")]),e._v(" "),n("li",[e._v("Runner\n"),n("ul",[n("li",[e._v("Pipelines runner: a set of DAG of pipelines (where each pipeline is schedule to run once all dependencies have been run)")])])]),e._v(" "),n("li",[e._v("Events => lead to new flows or pipelines being created … (or existing ones being stopped or destroyed)")])]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a5676e",graph:e.$dataBlock.mermaid_64a5676e}}),n("p",[e._v("State of Factory(flows, state)")]),e._v(" "),n("p",[e._v("f(event, state) => state")]),e._v(" "),n("p",[e._v("flow dependencies?")]),e._v(" "),n("p",[e._v("Desired properties")]),e._v(" "),n("ul",[n("li",[e._v("We economise on runs: we don’t rerun processors (pipelines?) that have same config and input data\n"),n("ul",[n("li",[e._v("=> one reason for breaking down into smaller “operators” is that we economise here …")])])]),e._v(" "),n("li",[e._v("Simplicity: the system is understandable …")]),e._v(" "),n("li",[e._v("Processors (pipelines) are atomic - they get their configuration and run …")]),e._v(" "),n("li",[e._v("We can generate from a source spec and an original datapackage.json a full set of pipelines / processors.")]),e._v(" "),n("li",[e._v("Pipelines as Templates vs Pipelines as instances …")])]),e._v(" "),n("p",[e._v("pipeline id = hash(pipeline spec, datapackage.json)")]),e._v(" "),n("p",[e._v("{pipelineid}/…")]),e._v(" "),n("p",[e._v("next pipeline can rely on {pipelineid}")]),e._v(" "),n("p",[e._v("Planner …")]),e._v(" "),n("p",[e._v("=> a pipeline is never rerun (once it is run)")]),e._v(" "),n("table",[n("thead",[n("tr",[n("th"),e._v(" "),n("th",[e._v("Factory")]),e._v(" "),n("th",[e._v("Airflow")])])]),e._v(" "),n("tbody",[n("tr",[n("td",[e._v("DAG")]),e._v(" "),n("td",[e._v("Implicit (no concept)")]),e._v(" "),n("td",[e._v("DAG")])]),e._v(" "),n("tr",[n("td",[e._v("Node")]),e._v(" "),n("td",[e._v("Pipelines (or processors?)")]),e._v(" "),n("td",[e._v("Operators")])]),e._v(" "),n("tr",[n("td",[e._v("Running Node")]),e._v(" "),n("td",[e._v("? Running pipelines")]),e._v(" "),n("td",[e._v("Tasks")])]),e._v(" "),n("tr",[n("td",[e._v("Comments")]),e._v(" "),n("td"),e._v(" "),n("td")])])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://airflow.incubator.apache.org/concepts.html#workflows",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://airflow.incubator.apache.org/concepts.html#workflows"),n("OutboundLink")],1)]),e._v(" "),n("h1",{attrs:{id:"analysis-from-work-for-preview"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#analysis-from-work-for-preview"}},[e._v("#")]),e._v(" Analysis from work for Preview")]),e._v(" "),n("p",[e._v("As a Publisher I want to upload a 50Mb CSV so that the showcase page works - it does not crash by browser (because it is trying to load and display 50Mb of CSV)")]),e._v(" "),n("p",[n("em",[e._v("plus")])]),e._v(" "),n("p",[e._v("As a Publisher I want to customize whether I generate a preview for a file or not so that I don’t get inappropriate previews")]),e._v(" "),n("blockquote",[n("p",[e._v("As a Publisher I want to have an SQLite version of my data auto-built")]),e._v(" "),n("p",[e._v("As a Publisher I want to upload an Excel file and have csv versions of each sheet and an sqlite version")])]),e._v(" "),n("h3",{attrs:{id:"overview"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#overview"}},[e._v("#")]),e._v(" Overview")]),e._v(" "),n("p",[n("em",[e._v("This is what we want")])]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a5599c",graph:e.$dataBlock.mermaid_64a5599c}}),n("h3",{attrs:{id:"how-does-this-work"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#how-does-this-work"}},[e._v("#")]),e._v(" How does this work?")]),e._v(" "),n("h4",{attrs:{id:"simple-example-no-previews-single-csv-in-source"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#simple-example-no-previews-single-csv-in-source"}},[e._v("#")]),e._v(" Simple example: no previews, single CSV in source")]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a55964",graph:e.$dataBlock.mermaid_64a55964}}),n("div",{staticClass:"language-yaml= extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("meta:\n  owner: <owner username>\n  ownerid: <owner unique id>\n  dataset: <dataset name>\n  version: 1\n  findability: <published/unlisted/private>\ninputs:\n -  # only one input is supported atm\n    kind: datapackage\n    url: <datapackage-url>\n    parameters:\n      resource-mapping:\n        <resource-name-or-path>: <resource-url>\n\noutputs:\n - ...  see https://github.com/datahq/pm/issues/17\n")])])]),n("h4",{attrs:{id:"with-previews"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#with-previews"}},[e._v("#")]),e._v(" With previews")]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a5595a",graph:e.$dataBlock.mermaid_64a5595a}}),n("h3",{attrs:{id:"with-excel-multiple-sheets"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#with-excel-multiple-sheets"}},[e._v("#")]),e._v(" With Excel (multiple sheets)")]),e._v(" "),n("p",[e._v("Source is vix-daily.xls (with 2 sheets)")]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a55922",graph:e.$dataBlock.mermaid_64a55922}}),n("p",[e._v("datapackage.json")]),e._v(" "),n("div",{staticClass:"language-javascript= extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('{\n  resources: [\n    {\n      "name": "mydata"\n      "path": "mydata.xls"\n    }\n  ]\n}\n')])])]),n("div",{staticClass:"language-yaml= extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("meta:\n  owner: <owner username>\n  ownerid: <owner unique id>\n  dataset: <dataset name>\n  version: 1\n  findability: <published/unlisted/private>\ninputs:\n -  # only one input is supported atm\n    kind: datapackage\n    url: <datapackage-url>\n    parameters:\n      resource-mapping:\n        <resource-name-or-path>: <resource-url>\n\nprocessing:\n -\n    input: <resource-name>   # mydata\n    output: <resource-name>  # mydata_sheet1\n    tabulator:\n        sheet: 1\n -\n    input: <resource-name>   # mydata\n    output: <resource-name>  # mydata_sheet2\n    tabulator:\n        sheet: 2\n")])])]),n("div",{staticClass:"language-yaml= extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("meta:\n  owner: <owner username>\n  ownerid: <owner unique id>\n  dataset: <dataset name>\n  version: 1\n  findability: <published/unlisted/private>\ninputs:\n -  # only one input is supported atm\n    kind: datapackage\n    url: <datapackage-url>\n    parameters:\n      resource-mapping:\n        <resource-name-or-path>: <resource-url> // excel file\n\n=> (implictly and in cli becomes ...)\n\n...\n\nprocessing:\n -\n    input: <resource-name>   # mydata\n    output: <resource-name>  # mydata-sheet1\n    tabulator:\n        sheet: 1\n")])])]),n("p",[e._v("Result")]),e._v(" "),n("div",{staticClass:"language-javascript= extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('{\n  resources: [\n    {\n      "name": "mydata",\n      "path": "mydata.xls"\n    },\n    {\n      "path": "derived/mydata.xls.sheet1.csv"\n      "datahub": {\n        "derivedFrom": "mydata"\n      }\n    },\n    {\n      "path": "derived/mydata.xls.sheet2.csv"\n      "datahub": {\n        "derivedFrom": "mydata"\n      }\n    }\n  ]\n}\n')])])]),n("h3",{attrs:{id:"overall-component-design"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#overall-component-design"}},[e._v("#")]),e._v(" Overall component design")]),e._v(" "),n("p",[e._v("Assembler …")]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a558ac",graph:e.$dataBlock.mermaid_64a558ac}}),n("p",[e._v("=> Assembler generates a DAG.")]),e._v(" "),n("ul",[n("li",[e._v("dest filenames in advance …")]),e._v(" "),n("li",[e._v("for each pipeline: pipelines it dpeends on\n"),n("ul",[n("li",[e._v("e.g. sqlite: depends on on all derived csv pipelines running")]),e._v(" "),n("li",[e._v("node depends: all csv, all json pipelines running")]),e._v(" "),n("li",[e._v("zip: depends on all csv running")])])]),e._v(" "),n("li",[e._v("Pipelines")])]),e._v(" "),n("Mermaid",{attrs:{id:"mermaid_64a552a2",graph:e.$dataBlock.mermaid_64a552a2}}),n("h3",{attrs:{id:"nts"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#nts"}},[e._v("#")]),e._v(" NTS")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("function(sourceSpec) => (pipelines, DAG)\n\npipeline\n\n  pipeline-id\n  steps = n *\n    processor\n    parameters\n  schedule\n  dependencies\n")])])])],1)}),[],!1,null,null,null);"function"==typeof s&&s(t);a.default=t.exports}}]);