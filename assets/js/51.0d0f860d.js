(window.webpackJsonp=window.webpackJsonp||[]).push([[51],{415:function(e,a,t){"use strict";t.r(a);var r=t(18),n=Object(r.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"data-flows-factory-research"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#data-flows-factory-research"}},[e._v("#")]),e._v(" Data Flows + Factory - Research")]),e._v(" "),t("h2",{attrs:{id:"nts"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#nts"}},[e._v("#")]),e._v(" NTS")]),e._v(" "),t("ul",[t("li",[e._v("Stream and Batch dichotomy is probably a false one – and unhelpful. Batch is just some grouping of stream. Batch done regularly enough starts to be a stream.")]),e._v(" "),t("li",[e._v("More useful is complete vs incomplete data sources")]),e._v(" "),t("li",[e._v("Hard part of streaming (or batch) work is handling case where events arrive “late”. For example, let’s say i want to total up total transaction volume at a bank per day … but some transactions arrived at the server late e.g. a transaction at 2355 actually arrives at 1207 because of network delay or some other issue then if i batch at 1200 based on what has arrived i have an issue. Most of work and complexity in Beam / DataFlow model relates to this.")]),e._v(" "),t("li",[e._v("Essential duality between flows and states via difference and wum. E.g. transaction and balance:\n"),t("ul",[t("li",[e._v("Balance over time – differenced --\x3e Flow")]),e._v(" "),t("li",[e._v("Flow – summed --\x3e Balance")])])]),e._v(" "),t("li",[e._v("Balance is often just a cached “sum”.")]),e._v(" "),t("li",[e._v("Also relevant to datsets: we often think of them as states but really they are a flow.")])]),e._v(" "),t("h2",{attrs:{id:"inbox"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#inbox"}},[e._v("#")]),e._v(" Inbox")]),e._v(" "),t("ul",[t("li",[e._v("[x] DataFlow paper: “The Dataflow Model: A Practical Approach to BalancingCorrectness, Latency, and Cost in Massive-Scale,Unbounded, Out-of-Order Data Processing” (2015)")]),e._v(" "),t("li",[e._v("[ ] Stream vs Batch\n"),t("ul",[t("li",[e._v("[x] Streaming 101: The world beyond batch. A high-level tour of modern data-processing concepts. (Aug 2015)  "),t("a",{attrs:{href:"https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101"),t("OutboundLink")],1),e._v(" "),t("strong",[e._v("Good intro to streaming and DataFlow by one of its authors")])]),e._v(" "),t("li",[e._v("[ ] "),t("a",{attrs:{href:"https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102"),t("OutboundLink")],1),e._v(" Follow up to previous paper")])])]),e._v(" "),t("li",[e._v("[ ] Apache Beam *"),t("em",[e._v("in progress – see below")])]),e._v(" "),t("li",[e._v("[ ] dbt "),t("strong",[e._v("initial review. Mainly a way conventient way of tracking in DB transforms")])]),e._v(" "),t("li",[e._v("[ ] Frictionless DataFlows")]),e._v(" "),t("li",[e._v("[x] Kreps (kafka author): "),t("a",{attrs:{href:"https://www.oreilly.com/radar/questioning-the-lambda-architecture/",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.oreilly.com/radar/questioning-the-lambda-architecture/"),t("OutboundLink")],1),e._v(" "),t("ul",[t("li",[e._v("lambda architecture is where you run both batch and streaming in parallel as way to have traditional processing plus some kind of real-time results.")]),e._v(" "),t("li",[e._v("basically Kreps says its a PITA to keep two parallel systems running and you can just go “streaming” (remember we are beyond the dichotomy)")])])])]),e._v(" "),t("h2",{attrs:{id:"apache-beam"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#apache-beam"}},[e._v("#")]),e._v(" Apache Beam")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://beam.apache.org/blog/2017/02/13/stateful-processing.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://beam.apache.org/blog/2017/02/13/stateful-processing.html"),t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"pipeline"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pipeline"}},[e._v("#")]),e._v(" Pipeline")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.pipeline.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.pipeline.html"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Pipeline, the top-level Beam object.")]),e._v(" "),t("p",[e._v("A pipeline holds a DAG of data transforms. Conceptually the nodes of the DAG are transforms (PTransform objects) and the edges are values (mostly PCollection objects). The transforms take as inputs one or more PValues and output one or more PValue s.")]),e._v(" "),t("p",[e._v("The pipeline offers functionality to traverse the graph. The actual operation to be executed for each node visited is specified through a runner object.")]),e._v(" "),t("p",[e._v("Typical usage:")]),e._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Create a pipeline object using a local runner for execution.")]),e._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("with")]),e._v(" beam"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("Pipeline"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[e._v("'DirectRunner'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("as")]),e._v(" p"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v("\n\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[e._v('# Add to the pipeline a "Create" transform. When executed this')]),e._v("\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# transform will produce a PCollection object with the specified values.")]),e._v("\n  pcoll "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" p "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[e._v("'Create'")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">>")]),e._v(" beam"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("Create"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[e._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[e._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[e._v("3")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Another transform could be applied to pcoll, e.g., writing to a text file.")]),e._v("\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# For other transforms, refer to transforms/ directory.")]),e._v("\n  pcoll "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[e._v("'Write'")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">>")]),e._v(" beam"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("io"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("WriteToText"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[e._v("'./output'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# run() will execute the DAG stored in the pipeline.  The execution of the")]),e._v("\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# nodes visited is done using the specified local runner.")]),e._v("\n")])])]),t("h2",{attrs:{id:"runners"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#runners"}},[e._v("#")]),e._v(" Runners")]),e._v(" "),t("ul",[t("li",[e._v("Airflow")]),e._v(" "),t("li",[e._v("Argo workflows")])]),e._v(" "),t("h3",{attrs:{id:"airflow"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#airflow"}},[e._v("#")]),e._v(" Airflow")]),e._v(" "),t("p",[e._v("Airflow organices tasks in a DAG. A DAG (Directed Acyclic Graph) is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.")]),e._v(" "),t("ul",[t("li",[e._v("Each task could be Bash, Python or others.")]),e._v(" "),t("li",[e._v("You can connect the tasks in a DAG as you want (which one depends on which).")]),e._v(" "),t("li",[e._v("Tasks could be built from Jinja templates.")]),e._v(" "),t("li",[e._v("It has a nice and comfortable UI.")])]),e._v(" "),t("p",[e._v("You can also use "),t("em",[e._v("Sensors")]),e._v(": you can wait for certain files or database changes for activate anoter jobs.")]),e._v(" "),t("p",[e._v("References")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://github.com/apache/airflow",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/apache/airflow"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://medium.com/videoamp/what-we-learned-migrating-off-cron-to-airflow-b391841a0da4",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://medium.com/videoamp/what-we-learned-migrating-off-cron-to-airflow-b391841a0da4"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://medium.com/@rbahaguejr/airflow-a-beautiful-cron-alternative-or-replacement-for-data-pipelines-b6fb6d0cddef",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://medium.com/@rbahaguejr/airflow-a-beautiful-cron-alternative-or-replacement-for-data-pipelines-b6fb6d0cddef"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"notes"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#notes"}},[e._v("#")]),e._v(" Notes")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://delta.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://delta.io/"),t("OutboundLink")],1),e._v(" - acid for data lakes")])]),e._v(" "),t("h3",{attrs:{id:"airtunnel"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#airtunnel"}},[e._v("#")]),e._v(" airtunnel")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/joerg-schneider/airtunnel",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/joerg-schneider/airtunnel"),t("OutboundLink")],1)]),e._v(" "),t("ul",[t("li",[t("p",[t("a",{attrs:{href:"https://medium.com/bcggamma/airtunnel-a-blueprint-for-workflow-orchestration-using-airflow-173054b458c3",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://medium.com/bcggamma/airtunnel-a-blueprint-for-workflow-orchestration-using-airflow-173054b458c3"),t("OutboundLink")],1),e._v(" - excellent piece on how to pattern airflow - “airtunnel”, plus overview of key tooling")]),e._v(" "),t("blockquote",[t("p",[e._v("This is why we postulate to have a central declaration file (as in YAML or JSON) per data asset, capturing all these properties required to run a generalized task (carried out by a custom operator). In other words, operators are designed in a generic way and receive the name of a data asset, from which they can grab its declaration file and learn how to parameterize and carry out the specific task.")])])])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("├── archive\n├── ingest\n│   ├── archive\n│   └── landing\n├── ready\n└── staging\n    ├── intermediate\n    ├── pickedup\n    └── ready\n")])])])])}),[],!1,null,null,null);a.default=n.exports}}]);